DESIGN:
First source of optimization is your algorithm
Second is Data model
Then comes the other

13
Unit test: you can isolate a single function (if it only does one task) and run it with good data and bad data: in this way you can test if the single function works good or not.

WHILE TODAY...:
muops = microoperations

Write non-obfuscated code:
Conditional branches are the killers of optimization!

Some C-specific hints: 

(Variable qualifiers)
NB: volatile variable can't be on a register

(Focus on the restrict qualifier)
Here we have no guarantee that a and b don't overlap! If we put restrict before a and b (as shown in the next slide) we are telling the compiler that there will be no memory overlap: in this way the compiler can perform some optimization

(Profile-guided optimization)
you can use gcc â€“fprofile-use (for example) to use the information taken in previous runs ( for example results of branches) to optimize your code

The cache memory:
Usually L1 is divided 32 KB for operations, 32 for data; L2 is usually unified between the two

Cache mapping (2nd):
The memory block number is needed to store the address of the block to which the cache is referring in this moment
(3rd):
Full mapping is very efficient in putting data in the cache, but is very inefficient in accessing them, because the cpu have to access all the blocks in the cache to find one single information: it could be anywhere!
Direct mapping is very efficient and fast in accessing (same address); nevertheless, it's not good in inserting data in memory: for example 7 and F must go both in 7, in the cache! So every time i put one of the two, I have to throw away the other one and so on; if these are counters in a loop it becomes a nightmare.
n-way associative is better; this is a n=2 example; normally the caches are 8-way associative.

The memory access pattern:
Actually, with the first miss you are filling the whole cache line; that's why it's a miss. In this way, the element 1 is loaded, so when you access it it's a hit; and so on with the other elements.

Strided access: [59]
We are jumping across memory! No problem if N is small, everything is in our cache line. If N is big, it's a problem. Would it be better to have strided access on the original matrix? YES! Strided writes are more expensive than strided loads.
The writeback to memory wouldn't happen if you were writing on the same line

Graph: [63]
very big difference!! (log scale)

Cache-associativity conflicts:
The last 6 bits of the address are for the inline position. And then, with a given number of sets (N = 2^S), we have in the address, before the bits about the inline position, S bits to indicate the set position.

Hot & cold fields:
If you have a very long linked list, you will more often move to another node than call the function do_something.
Since the key object of the struct is only one value, when i load it I load all the line. If we put the pointer object right after the double, we will already have it in cache, and it will be easier to access it. PROXIMITY

[big skip]

The cost of branch mis-prediction: [112]
NB: a for is normally forecasted as true, an if is normally forecasted as false; you can change this. How? We will see.

Branch prediction: [141]
In the functions version (see code loop.c) the optimization is done by the system, because he recognizes that a lot of branches are fixed decisions.

146
You could here do the verification of the condition outside of the for loop. Probably faster to do as the code on the slide, because the branch predictor can do the optimization for you since he can understand that there are a lot of 1 and then a lot of 0

147
This is a shifting of the bits; like an integer division for 2 (or multiplication if the shifting is done on the other sense)
In this case i'm shifting by 31 positions
Whatever is the data[ii] value, they are all shifted to the right. If the number is negative, all the bits will be 1, so the number is -1 in binary coding.
Then I perform a logical AND with the negation of t; if we AND with all zeros, the number is 0 so i'm not summing up anything (I'm in the case that data[ii] is not greater than pivot, so i don't want to sum) and viceversa

150
Calculating both branches

151
The vectorization is done in the following way: with little numbers we can do, in one operation, the condition and sum of all the numbers in this way:
(numbers, ..., )>(P,P,P,P,P,P)
And then multiply the result back to the numbers: 2 operations, now we can do the sum on all the elements of the final vector. Note that this can't be done with difficult condition operations.

152
This is not really a branch, it's just an evaluation

156
The fact that in the second case we have 27 in both is due to the fact that the data are unpredictable: failed branch prediction. The code is not stable; in the second case (next slide), it is stable